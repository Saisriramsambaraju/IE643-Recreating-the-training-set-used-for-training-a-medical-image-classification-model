{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465a0f67",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:06.731860Z",
     "iopub.status.busy": "2024-11-02T21:50:06.730974Z",
     "iopub.status.idle": "2024-11-02T21:50:18.652214Z",
     "shell.execute_reply": "2024-11-02T21:50:18.651377Z"
    },
    "papermill": {
     "duration": 11.934458,
     "end_time": "2024-11-02T21:50:18.654696",
     "exception": false,
     "start_time": "2024-11-02T21:50:06.720238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d074f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:18.672808Z",
     "iopub.status.busy": "2024-11-02T21:50:18.672146Z",
     "iopub.status.idle": "2024-11-02T21:50:18.792744Z",
     "shell.execute_reply": "2024-11-02T21:50:18.791698Z"
    },
    "papermill": {
     "duration": 0.132048,
     "end_time": "2024-11-02T21:50:18.795008",
     "exception": false,
     "start_time": "2024-11-02T21:50:18.662960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)  # Set the current device to the first GPU\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VGG16_MRI(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16_MRI, self).__init__()\n",
    "        # Load a pre-trained VGG16 model with batch normalization\n",
    "        model = torchvision.models.vgg16_bn(pretrained=True)\n",
    "        \n",
    "        # Change the first convolutional layer to accept single-channel (grayscale) input\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        # Retain the feature extraction layers\n",
    "        self.feature = model.features\n",
    "        \n",
    "        # Define the feature dimension based on output size for 240x240 input\n",
    "        # VGG16 feature output will be (512, 7, 7) for 224x224, so we calculate for 240x240\n",
    "        self.feat_dim = 512 * 7 * 7  # Update this if output size changes with input size\n",
    "        \n",
    "        # Adjust the number of classes for binary classification (0 or 1)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Batch normalization layer\n",
    "        self.bn = nn.BatchNorm1d(self.feat_dim)\n",
    "        self.bn.bias.requires_grad_(False)  # no shift\n",
    "        \n",
    "        # Fully connected layer to map features to the number of classes\n",
    "        self.fc_layer = nn.Linear(self.feat_dim, self.num_classes)\n",
    "        \n",
    "        self.model = model\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Pass input through feature extraction layers\n",
    "        feature = self.feature(x)\n",
    "        feature = feature.view(feature.size(0), -1)  # Flatten the feature map\n",
    "        feature = self.bn(feature)  # Apply batch normalization\n",
    "        res = self.fc_layer(feature)  # Output class scores\n",
    "        \n",
    "        return feature, res\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Pass input through feature extraction layers\n",
    "        feature = self.feature(x)\n",
    "        feature = feature.view(feature.size(0), -1)  # Flatten the feature map\n",
    "        feature = self.bn(feature)  # Apply batch normalization\n",
    "        res = self.fc_layer(feature)  # Output class scores\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29d6848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:18.813239Z",
     "iopub.status.busy": "2024-11-02T21:50:18.812898Z",
     "iopub.status.idle": "2024-11-02T21:50:19.007712Z",
     "shell.execute_reply": "2024-11-02T21:50:19.006675Z"
    },
    "papermill": {
     "duration": 0.206888,
     "end_time": "2024-11-02T21:50:19.009936",
     "exception": false,
     "start_time": "2024-11-02T21:50:18.803048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/preprocessed-brats23/labels.csv\")\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = \"/kaggle/input/preprocessed-brats23/Images\"\n",
    "\n",
    "# Split into train, validation, and test sets \n",
    "train_df, temp_df = train_test_split(labels_df, test_size=0.3, stratify=labels_df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48d5b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:19.027558Z",
     "iopub.status.busy": "2024-11-02T21:50:19.027221Z",
     "iopub.status.idle": "2024-11-02T21:50:19.038826Z",
     "shell.execute_reply": "2024-11-02T21:50:19.037882Z"
    },
    "papermill": {
     "duration": 0.022626,
     "end_time": "2024-11-02T21:50:19.040794",
     "exception": false,
     "start_time": "2024-11-02T21:50:19.018168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_dir, self.df.iloc[idx]['filename'])\n",
    "        image = Image.open(img_name).convert('L')  # Convert to grayscale\n",
    "        label = int(self.df.iloc[idx]['label'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define image transformations (normalization can be adjusted based on data needs)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),  # Ensure image size is 240x240\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MRIDataset(train_df, data_dir, transform=transform)\n",
    "val_dataset = MRIDataset(val_df, data_dir, transform=transform)\n",
    "test_dataset = MRIDataset(test_df, data_dir, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565e25fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:19.057661Z",
     "iopub.status.busy": "2024-11-02T21:50:19.057115Z",
     "iopub.status.idle": "2024-11-02T21:50:28.200185Z",
     "shell.execute_reply": "2024-11-02T21:50:28.199372Z"
    },
    "papermill": {
     "duration": 9.154044,
     "end_time": "2024-11-02T21:50:28.202601",
     "exception": false,
     "start_time": "2024-11-02T21:50:19.048557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
      "100%|██████████| 528M/528M [00:06<00:00, 81.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = VGG16_MRI(num_classes=2).to('cuda')  # Use GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train() :\n",
    "    num_epochs = 10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to('cuda')\n",
    "    \n",
    "            labels = labels.to('cuda')\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            _, outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "    \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to('cuda')\n",
    "                labels = labels.to('cuda')\n",
    "                \n",
    "                _, outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation loss and accuracy\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "    pretrained_VGG_MRI_model = model\n",
    "    torch.save(model.state_dict(), '/kaggle/working/classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f26055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.228625Z",
     "iopub.status.busy": "2024-11-02T21:50:28.228275Z",
     "iopub.status.idle": "2024-11-02T21:50:28.233151Z",
     "shell.execute_reply": "2024-11-02T21:50:28.232271Z"
    },
    "papermill": {
     "duration": 0.020013,
     "end_time": "2024-11-02T21:50:28.235214",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.215201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pretrained_classifier(path=None):\n",
    "    if path is None:\n",
    "        path = \"/kaggle/input/brats23-classifier/pytorch/default/1/classifier.pt\"\n",
    "    model = VGG16_MRI(num_classes=2)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087b020d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.259875Z",
     "iopub.status.busy": "2024-11-02T21:50:28.259539Z",
     "iopub.status.idle": "2024-11-02T21:50:28.264259Z",
     "shell.execute_reply": "2024-11-02T21:50:28.263325Z"
    },
    "papermill": {
     "duration": 0.019303,
     "end_time": "2024-11-02T21:50:28.266271",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.246968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory_path):\n",
    "    try:\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "        print(f\"Directory created successfully: {directory_path}\")\n",
    "    except OSError as error:\n",
    "        print(f\"Error creating directory: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5961186b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.290890Z",
     "iopub.status.busy": "2024-11-02T21:50:28.290606Z",
     "iopub.status.idle": "2024-11-02T21:50:28.295296Z",
     "shell.execute_reply": "2024-11-02T21:50:28.294346Z"
    },
    "papermill": {
     "duration": 0.019385,
     "end_time": "2024-11-02T21:50:28.297265",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.277880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "def freeze(net):\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(False) \n",
    "\n",
    "def unfreeze(net):\n",
    "    for p in net.parameters():\n",
    "        p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97beb60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.321771Z",
     "iopub.status.busy": "2024-11-02T21:50:28.321504Z",
     "iopub.status.idle": "2024-11-02T21:50:28.330869Z",
     "shell.execute_reply": "2024-11-02T21:50:28.330167Z"
    },
    "papermill": {
     "duration": 0.023728,
     "end_time": "2024-11-02T21:50:28.332697",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.308969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim=100, dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2, padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU())\n",
    "        \n",
    "        # Fully connected layer to expand noise to a larger size\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim * 8 * 15 * 15, bias=False),\n",
    "            nn.BatchNorm1d(dim * 8 * 15 * 15),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Deconvolutional layers for upsampling to 240x240\n",
    "        self.l2_5 = nn.Sequential(\n",
    "            dconv_bn_relu(dim * 8, dim * 4),   # 15x15 -> 30x30\n",
    "            dconv_bn_relu(dim * 4, dim * 2),   # 30x30 -> 60x60\n",
    "            dconv_bn_relu(dim * 2, dim),       # 60x60 -> 120x120\n",
    "            nn.ConvTranspose2d(dim, 1, 5, 2, padding=2, output_padding=1),  # 120x120 -> 240x240\n",
    "            nn.Sigmoid())  # Output pixel values in range [0, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 15, 15)\n",
    "        y = self.l2_5(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a101a7cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.358575Z",
     "iopub.status.busy": "2024-11-02T21:50:28.358239Z",
     "iopub.status.idle": "2024-11-02T21:50:28.372990Z",
     "shell.execute_reply": "2024-11-02T21:50:28.372206Z"
    },
    "papermill": {
     "duration": 0.029986,
     "end_time": "2024-11-02T21:50:28.375029",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.345043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discriminator discri.py \n",
    "class MinibatchDiscrimination(nn.Module):\n",
    "    def __init__(self, in_features, out_features, kernel_dims, mean=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.kernel_dims = kernel_dims\n",
    "        self.mean = mean\n",
    "        self.T = nn.Parameter(torch.Tensor(in_features, out_features, kernel_dims))\n",
    "        init.normal(self.T, 0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is NxA\n",
    "        # T is AxBxC\n",
    "        matrices = x.mm(self.T.view(self.in_features, -1))\n",
    "        matrices = matrices.view(-1, self.out_features, self.kernel_dims)\n",
    "\n",
    "        M = matrices.unsqueeze(0)  # 1xNxBxC\n",
    "        M_T = M.permute(1, 0, 2, 3)  # Nx1xBxC\n",
    "        norm = torch.abs(M - M_T).sum(3)  # NxNxB\n",
    "        expnorm = torch.exp(-norm)\n",
    "        o_b = (expnorm.sum(0) - 1)   # NxB, subtract self distance\n",
    "        if self.mean:\n",
    "            o_b /= x.size(0) - 1\n",
    "\n",
    "        x = torch.cat([x, o_b], 1)\n",
    "        return x\n",
    "\n",
    "class MinibatchDiscriminator(nn.Module):\n",
    "    def __init__(self,in_dim=1, dim=64, n_classes=1000):\n",
    "        super(MinibatchDiscriminator, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        def conv_ln_lrelu(in_dim, out_dim, k, s, p):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim, out_dim, k, s, p),\n",
    "                # Since there is no effective implementation of LayerNorm,\n",
    "                # we use InstanceNorm2d instead of LayerNorm here.\n",
    "                nn.InstanceNorm2d(out_dim, affine=True),\n",
    "                nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.layer1 = conv_ln_lrelu(in_dim, dim, 5, 2, 2)\n",
    "        self.layer2 = conv_ln_lrelu(dim, dim*2, 5, 2, 2)\n",
    "        self.layer3 = conv_ln_lrelu(dim*2, dim*4, 5, 2, 2)\n",
    "        self.layer4 = conv_ln_lrelu(dim*4, dim*4, 3, 2, 1)\n",
    "        self.mbd1 = MinibatchDiscrimination(57600, 64, 50)\n",
    "        self.fc_layer = nn.Linear(57600+64, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        bs = x.shape[0]\n",
    "        feat1 = self.layer1(x)\n",
    "        out.append(feat1)\n",
    "        feat2 = self.layer2(feat1)\n",
    "        out.append(feat2)\n",
    "        feat3 = self.layer3(feat2)\n",
    "        out.append(feat3)\n",
    "        feat4 = self.layer4(feat3)\n",
    "        out.append(feat4)\n",
    "        feat = feat4.view(bs, -1)\n",
    "        # print('feat:', feat.shape)\n",
    "        mb_out = self.mbd1(feat)   # Nx(A+B)\n",
    "        y = self.fc_layer(mb_out)\n",
    "        \n",
    "        return feat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a86a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.399890Z",
     "iopub.status.busy": "2024-11-02T21:50:28.399126Z",
     "iopub.status.idle": "2024-11-02T21:50:28.406948Z",
     "shell.execute_reply": "2024-11-02T21:50:28.406251Z"
    },
    "papermill": {
     "duration": 0.022155,
     "end_time": "2024-11-02T21:50:28.408943",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.386788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_GAN(n_classes, z_dim, Pretrained = False):\n",
    "\n",
    "    # if Pretrained :\n",
    "        # G= \n",
    "    # else :\n",
    "    G = Generator(z_dim)\n",
    "    D = MinibatchDiscriminator(n_classes=n_classes)\n",
    "    \n",
    "    G = torch.nn.DataParallel(G).to(device)\n",
    "    D = torch.nn.DataParallel(D).to(device)\n",
    "    if Pretrained:\n",
    "        root_path = \"/kaggle/input/brats23-gan-epoch75/pytorch/default/1/attack_results\"\n",
    "        dataset_name = \"BraTS23\"\n",
    "        mode_name_T = \"VGG16_MRI\"\n",
    "        path = os.path.join(root_path, os.path.join(dataset_name, model_name_T))\n",
    "        # path = os.path.join(os.path.join(gan_model_dir, dataset), target_model)\n",
    "        path_G = os.path.join(path, \"ep75_improved_{}_G.pt\".format(dataset_name))\n",
    "        path_D = os.path.join(path, \"ep75_improved_{}_D.pt\".format(dataset_name))\n",
    "        ckp_G = torch.load(path_G)\n",
    "        G.load_state_dict(ckp_G['state_dict'], strict=True)\n",
    "        ckp_D = torch.load(path_D)\n",
    "        D.load_state_dict(ckp_D['state_dict'], strict=True)\n",
    "        print(\"Loaded Pretrained Model (Specific GAN)\")\n",
    "    \n",
    "    return G, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15709cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.433646Z",
     "iopub.status.busy": "2024-11-02T21:50:28.433330Z",
     "iopub.status.idle": "2024-11-02T21:50:28.437741Z",
     "shell.execute_reply": "2024-11-02T21:50:28.436978Z"
    },
    "papermill": {
     "duration": 0.018944,
     "end_time": "2024-11-02T21:50:28.439708",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.420764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_augmodel():\n",
    "    # model = pretrained_VGG_MRI_model\n",
    "    model = load_pretrained_classifier()\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72acab81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.464065Z",
     "iopub.status.busy": "2024-11-02T21:50:28.463767Z",
     "iopub.status.idle": "2024-11-02T21:50:28.472073Z",
     "shell.execute_reply": "2024-11-02T21:50:28.471228Z"
    },
    "papermill": {
     "duration": 0.022852,
     "end_time": "2024-11-02T21:50:28.473999",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.451147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def init_dataloader(df = None, data_dir=\"/kaggle/input/preprocessed-brats23/Images\", batch_size=64, mode=\"gan\", transform=None, iterator=False):\n",
    "    tf = time.time()\n",
    "    if df is None : \n",
    "        df = pd.read_csv(\"/kaggle/input/preprocessed-brats23/labels.csv\")\n",
    "        df,_ = train_test_split(labels_df, test_size=0.4, stratify=labels_df['label'], random_state=42)\n",
    "    # Define shuffle based on mode (assuming \"attack\" mode does not shuffle data)\n",
    "    shuffle_flag = False if mode == \"attack\" else True\n",
    "\n",
    "    # Initialize the dataset with the MRIDataset class\n",
    "    # Define image transformations (normalization can be adjusted based on data needs)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((240, 240)),  # Ensure image size is 240x240\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = MRIDataset(df=df, data_dir=data_dir, transform=transform)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    if iterator:\n",
    "        data_loader = DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=shuffle_flag,\n",
    "                                 drop_last=True,\n",
    "                                 num_workers=0,\n",
    "                                 pin_memory=True).__iter__()\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=shuffle_flag,\n",
    "                                 drop_last=True,\n",
    "                                 num_workers=2,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    interval = time.time() - tf\n",
    "    print(f'Initializing data loader took {interval:.2f} seconds')\n",
    "    \n",
    "    return dataset, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac6c436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.498333Z",
     "iopub.status.busy": "2024-11-02T21:50:28.498045Z",
     "iopub.status.idle": "2024-11-02T21:50:28.504761Z",
     "shell.execute_reply": "2024-11-02T21:50:28.503833Z"
    },
    "papermill": {
     "duration": 0.020954,
     "end_time": "2024-11-02T21:50:28.506619",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.485665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_act_reg(train_loader,T,device,Nsample=5000):\n",
    "    all_fea = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(train_loader): # batchsize =100\n",
    "            # print(data.shape)\n",
    "            data,_ = data\n",
    "            if batch_idx*len(data) > Nsample:\n",
    "                break\n",
    "            data  = data.to(device)\n",
    "            fea,_ = T(data)\n",
    "            if batch_idx == 0:\n",
    "                all_fea = fea\n",
    "            else:\n",
    "                all_fea = torch.cat((all_fea,fea))\n",
    "    fea_mean = torch.mean(all_fea,dim=0)\n",
    "    fea_logvar = torch.std(all_fea,dim=0)\n",
    "    \n",
    "    print(fea_mean.shape, fea_logvar.shape, all_fea.shape)\n",
    "    return fea_mean,fea_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "746cb9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.531009Z",
     "iopub.status.busy": "2024-11-02T21:50:28.530751Z",
     "iopub.status.idle": "2024-11-02T21:50:28.539078Z",
     "shell.execute_reply": "2024-11-02T21:50:28.538250Z"
    },
    "papermill": {
     "duration": 0.022814,
     "end_time": "2024-11-02T21:50:28.540919",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.518105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get_attack_model (utils.py)\n",
    "def get_attack_model(eval_mode=False):\n",
    "    n_classes=2\n",
    "\n",
    "    G, D = get_GAN(n_classes=n_classes,z_dim=100)\n",
    "\n",
    "    dataset = \"BraTS23\"\n",
    "    cid = [0]\n",
    "    # target and student classifiers\n",
    "    for i in range(len(cid)):\n",
    "        model = get_augmodel()\n",
    "        model = model.to(device)\n",
    "        model = model.eval()\n",
    "        if i==0:\n",
    "            targetnets = [model]\n",
    "        else:\n",
    "            targetnets.append(model)\n",
    "    \n",
    "        # p_reg \n",
    "        # if args.loss=='logit_loss: \n",
    "        if True :\n",
    "            # if model_types_[id_] == \"IR152\" or model_types_[id_]==\"VGG16\" or model_types_[id_]==\"FaceNet64\": \n",
    "                #target model\n",
    "#             create_directory_if_not_exists(\"/kaggle/working/checkpoints/p_reg\")\n",
    "#             p_reg = os.path.join(\"/kaggle/working/checkpoints/p_reg\", '{}_{}_p_reg.pt'.format(dataset,\"VGG16_MRI\")) #'./p_reg/{}_{}_p_reg.pt'.format(dataset,model_types_[id_])\n",
    "#             if not os.path.exists(p_reg):\n",
    "            data_dir = \"/kaggle/input/preprocessed-brats23/Images\"\n",
    "            _, dataloader_gan = init_dataloader(df=None,data_dir = data_dir)\n",
    "                # from attack import get_act_reg\n",
    "#                 if os.path.isdir(p_reg):\n",
    "#                     raise ValueError(f\"Expected {p_reg} to be a file, but found a directory instead.\")\n",
    "            fea_mean_,fea_logvar_ = get_act_reg(dataloader_gan,model,device)\n",
    "#                 torch.save({'fea_mean':fea_mean_,'fea_logvar':fea_logvar_},p_reg)\n",
    "#             else:\n",
    "#                 fea_reg = torch.load(p_reg)\n",
    "#                 fea_mean_ = fea_reg['fea_mean']\n",
    "#                 fea_logvar_ = fea_reg['fea_logvar']\n",
    "            if i == 0:\n",
    "                fea_mean = [fea_mean_.to(device)]\n",
    "                fea_logvar = [fea_logvar_.to(device)]\n",
    "#             else:\n",
    "#                 fea_mean.append(fea_mean_)\n",
    "#                 fea_logvar.append(fea_logvar_)\n",
    "            # print('fea_logvar_',i,fea_logvar_.shape,fea_mean_.shape)\n",
    "            \n",
    "        # else:\n",
    "        #     fea_mean,fea_logvar = 0,0\n",
    "    \n",
    "    # evaluation classifier\n",
    "    E = get_augmodel()\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "\n",
    "    return targetnets, E, G, D, n_classes, fea_mean, fea_logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54c65483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.565285Z",
     "iopub.status.busy": "2024-11-02T21:50:28.565018Z",
     "iopub.status.idle": "2024-11-02T21:50:28.574213Z",
     "shell.execute_reply": "2024-11-02T21:50:28.573457Z"
    },
    "papermill": {
     "duration": 0.023818,
     "end_time": "2024-11-02T21:50:28.576144",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.552326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attack.py\n",
    "def reparameterize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Reparameterization trick to sample from N(mu, var) from\n",
    "    N(0,1).\n",
    "    :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "    :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "    :return: (Tensor) [B x D]\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "\n",
    "    return eps * std + mu\n",
    "\n",
    "def reg_loss(featureT,fea_mean, fea_logvar):\n",
    "    \n",
    "    fea_reg = reparameterize(fea_mean, fea_logvar)\n",
    "    fea_reg = fea_mean.repeat(featureT.shape[0],1)\n",
    "    loss_reg = torch.mean((featureT - fea_reg).pow(2))\n",
    "    # print('loss_reg',loss_reg)\n",
    "    return loss_reg\n",
    "\n",
    "def iden_loss(T,fake, iden, used_loss,criterion,fea_mean=0, fea_logvar=0,lam=0.1):\n",
    "    Iden_Loss = 0\n",
    "    loss_reg = 0\n",
    "    for tn in T:\n",
    "      \n",
    "        feat,out = tn(fake)\n",
    "        if used_loss == 'logit_loss': #reg only with the target classifier, reg is randomly from distribution\n",
    "            if Iden_Loss ==0:                \n",
    "                loss_sdt =  criterion(out, iden)\n",
    "                loss_reg = lam*reg_loss(feat,fea_mean[0], fea_logvar[0]) #reg only with the target classifier\n",
    "\n",
    "                Iden_Loss = Iden_Loss + loss_sdt  \n",
    "            else:                \n",
    "                loss_sdt =  criterion(out, iden)\n",
    "                Iden_Loss = Iden_Loss + loss_sdt\n",
    "\n",
    "        else:\n",
    "            loss_sdt = criterion(out, iden)\n",
    "            Iden_Loss = Iden_Loss + loss_sdt\n",
    "\n",
    "    Iden_Loss = Iden_Loss/len(T) + loss_reg\n",
    "    return Iden_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40417789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.600867Z",
     "iopub.status.busy": "2024-11-02T21:50:28.600608Z",
     "iopub.status.idle": "2024-11-02T21:50:28.605981Z",
     "shell.execute_reply": "2024-11-02T21:50:28.605040Z"
    },
    "papermill": {
     "duration": 0.019614,
     "end_time": "2024-11-02T21:50:28.607765",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.588151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_criterion(used_loss):\n",
    "    criterion = None\n",
    "    if used_loss=='logit_loss':\n",
    "        criterion = nn.NLLLoss().to(device)\n",
    "        print('criterion:{}'.format(used_loss))\n",
    "    elif used_loss=='cel':\n",
    "        criterion = nn.CrossEntropyLoss().to(device)    \n",
    "        print('criterion',criterion)\n",
    "    else:\n",
    "        print('criterion:{}'.format(used_loss))\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a69ed43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.632023Z",
     "iopub.status.busy": "2024-11-02T21:50:28.631745Z",
     "iopub.status.idle": "2024-11-02T21:50:28.639100Z",
     "shell.execute_reply": "2024-11-02T21:50:28.638216Z"
    },
    "papermill": {
     "duration": 0.021742,
     "end_time": "2024-11-02T21:50:28.641093",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.619351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deprocessor():\n",
    "    # resize 240,240\n",
    "    proc = []\n",
    "    proc.append(transforms.Resize((240, 240)))\n",
    "    proc.append(transforms.ToTensor())\n",
    "    return transforms.Compose(proc)\n",
    "\n",
    "\n",
    "def low2high(img):\n",
    "    # Convert from low to high resolution, for grayscale images (1 channel)\n",
    "    bs = img.size(0)  # Batch size\n",
    "    proc = get_deprocessor()  # Preprocessing for resizing and tensor conversion\n",
    "    img_tensor = img.detach().cpu().float()  # Detach and move to CPU, ensure it's a float tensor\n",
    "    \n",
    "    # Create a tensor to hold the upscaled images, with 1 channel for grayscale\n",
    "    img = torch.zeros(bs, 1, 240, 240)  # Change from 3 to 1 channel for grayscale\n",
    "    \n",
    "    for i in range(bs):\n",
    "        # Convert tensor to PIL grayscale image (no RGB conversion)\n",
    "        img_i = transforms.ToPILImage()(img_tensor[i, :, :, :]).convert('L')  # 'L' mode for grayscale\n",
    "        img_i = proc(img_i)  # Apply the deprocessing (resize and convert back to tensor)\n",
    "        img[i, :, :, :] = img_i[:, :, :]  # Assign to output tensor\n",
    "    \n",
    "    img = img.cuda()  # Move back to GPU (if available)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a693cc7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.666641Z",
     "iopub.status.busy": "2024-11-02T21:50:28.665953Z",
     "iopub.status.idle": "2024-11-02T21:50:28.685845Z",
     "shell.execute_reply": "2024-11-02T21:50:28.684926Z"
    },
    "papermill": {
     "duration": 0.034879,
     "end_time": "2024-11-02T21:50:28.687793",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.652914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def dist_inversion(G, D, T, E, iden, lr=2e-2, momentum=0.9, lamda=100, \\\n",
    "                   iter_times=1500, clip_range=1.0, improved=False, num_seeds=5, \\\n",
    "                   used_loss='cel', prefix='', random_seed=0, save_img_dir='',fea_mean=0, \\\n",
    "                   fea_logvar=0, lam=0.1, clipz=False):\n",
    "    \n",
    "    iden = iden.view(-1).long().to(device)\n",
    "    criterion = find_criterion(used_loss)\n",
    "    bs = iden.shape[0]\n",
    "    \n",
    "    G.eval() \n",
    "    D.eval()\n",
    "    E.eval()\n",
    "    \n",
    "    #NOTE\n",
    "    mu = Variable(torch.zeros(bs, 100), requires_grad=True)\n",
    "    log_var = Variable(torch.ones(bs, 100), requires_grad=True)\n",
    "    \n",
    "    params = [mu, log_var]\n",
    "    solver = optim.Adam(params, lr=lr)\n",
    "    outputs_z = \"{}_iter_{}_{}_dis.npy\".format(prefix, random_seed, iter_times-1)\n",
    "    \n",
    "    if not os.path.exists(outputs_z):\n",
    "        outputs_z = \"{}_iter_{}_{}_dis\".format(prefix, random_seed, 0)\n",
    "        outputs_label = \"{}_iter_{}_{}_label\".format(prefix, random_seed, 0)\n",
    "        np.save(outputs_z,{\"mu\":mu.detach().cpu().numpy(),\"log_var\":log_var.detach().cpu().numpy()})\n",
    "        np.save(outputs_label,iden.detach().cpu().numpy())\n",
    "            \n",
    "        for i in range(iter_times):\n",
    "            z = reparameterize(mu, log_var)\n",
    "            if clipz==True:\n",
    "                z =  torch.clamp(z,-clip_range,clip_range).float()\n",
    "            fake = G(z)\n",
    "\n",
    "            if improved == True:\n",
    "                _, label =  D(fake)\n",
    "            else:\n",
    "                label = D(fake)\n",
    "                    \n",
    "            for p in params:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.zero_()\n",
    "            Iden_Loss = iden_loss(T,fake, iden, used_loss, criterion, fea_mean, fea_logvar, lam)\n",
    "\n",
    "            if improved:\n",
    "                Prior_Loss = torch.mean(F.softplus(log_sum_exp(label))) - torch.mean(log_sum_exp(label))\n",
    "            else:\n",
    "                Prior_Loss = - label.mean()\n",
    "\n",
    "            Total_Loss = Prior_Loss + lamda * Iden_Loss\n",
    "           \n",
    "            Total_Loss.backward()\n",
    "            solver.step()\n",
    "\n",
    "            Prior_Loss_val = Prior_Loss.item()\n",
    "            Iden_Loss_val = Iden_Loss.item()\n",
    "\n",
    "            if (i+1) % 300 == 0:\n",
    "                outputs_z = \"{}_iter_{}_{}_dis\".format(prefix, random_seed, i)\n",
    "                outputs_label = \"{}_iter_{}_{}_label\".format(prefix, random_seed, i)\n",
    "                np.save(outputs_z,{\"mu\":mu.detach().cpu().numpy(),\"log_var\":log_var.detach().cpu().numpy()})\n",
    "                np.save(outputs_label,iden.detach().cpu().numpy())\n",
    "        \n",
    "                with torch.no_grad():\n",
    "                    z = reparameterize(mu, log_var)\n",
    "                    if clipz==True:\n",
    "                        z =  torch.clamp(z,-clip_range, clip_range).float()\n",
    "                    fake_img = G(z.detach())\n",
    "                    eval_prob = E(low2high(fake_img))[-1]\n",
    "                    \n",
    "                    eval_iden = torch.argmax(eval_prob, dim=1).view(-1)\n",
    "                    acc = iden.eq(eval_iden.long()).sum().item() * 100.0 / bs\n",
    "                    save_tensor_images(fake_img, save_img_dir + '{}.png'.format(i+1))\n",
    "                    print(\"Iteration:{}\\tPrior Loss:{:.2f}\\tIden Loss:{:.2f}\\tAttack Acc:{:.2f}\".format(i+1, Prior_Loss_val, Iden_Loss_val, acc))\n",
    "                    \n",
    "                        \n",
    "        outputs_z = \"{}_iter_{}_{}_dis\".format(prefix, random_seed, iter_times)\n",
    "        outputs_label = \"{}_iter_{}_{}_label\".format(prefix, random_seed, iter_times)\n",
    "        np.save(outputs_z,{\"mu\":mu.detach().cpu().numpy(),\"log_var\":log_var.detach().cpu().numpy()})\n",
    "        np.save(outputs_label,iden.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2713846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.713678Z",
     "iopub.status.busy": "2024-11-02T21:50:28.713091Z",
     "iopub.status.idle": "2024-11-02T21:50:28.741329Z",
     "shell.execute_reply": "2024-11-02T21:50:28.740645Z"
    },
    "papermill": {
     "duration": 0.043149,
     "end_time": "2024-11-02T21:50:28.743310",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.700161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.utils as tvls\n",
    "def save_tensor_images(images, filename, nrow = None, normalize = True):\n",
    "    if not nrow:\n",
    "        tvls.save_image(images, filename, normalize = normalize, padding=0)\n",
    "    else:\n",
    "        tvls.save_image(images, filename, normalize = normalize, nrow=nrow, padding=0)\n",
    "\n",
    "class HLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = -1.0 * b.sum()\n",
    "        return b\n",
    "\n",
    "# define \"soft\" cross-entropy with pytorch tensor operations\n",
    "def softXEnt (input, target):\n",
    "    targetprobs = nn.functional.softmax (target, dim = 1)\n",
    "    logprobs = nn.functional.log_softmax (input, dim = 1)\n",
    "    return  -(targetprobs * logprobs).sum() / input.shape[0]\n",
    "\n",
    "def log_sum_exp(x, axis = 1):\n",
    "    m = torch.max(x, dim = 1)[0]\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(1)), dim = axis))\n",
    "\n",
    "def train_specific_gan():\n",
    "\n",
    "    # Hyperparams\n",
    "    file_path = None\n",
    "    model_name_T = \"VGG16_MRI\"\n",
    "    lr = 0.0002\n",
    "    batch_size = 64\n",
    "    z_dim = 100\n",
    "    epochs = 25\n",
    "    n_critic = 5\n",
    "    dataset_name = \"BraTS23\"\n",
    "    \n",
    "\n",
    "    # Create save folders\n",
    "    root_path = \"/kaggle/working/attack_results\"\n",
    "    save_model_dir = os.path.join(root_path, os.path.join(dataset_name, model_name_T))\n",
    "    save_img_dir = os.path.join(save_model_dir, \"imgs\")\n",
    "    os.makedirs(save_model_dir, exist_ok=True)\n",
    "    os.makedirs(save_img_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Load target model\n",
    "    T = get_augmodel()\n",
    "\n",
    "    # Dataset\n",
    "    data_dir = \"/kaggle/input/preprocessed-brats23/Images\"\n",
    "    dataset, dataloader = init_dataloader(df=None,data_dir=data_dir,batch_size=batch_size)\n",
    "    \n",
    "    # Start Training\n",
    "    print(\"Training GAN for %s\" % model_name_T)\n",
    "\n",
    "    G = Generator(z_dim)\n",
    "    DG = MinibatchDiscriminator(n_classes = 2)\n",
    "    \n",
    "    G = torch.nn.DataParallel(G).cuda()\n",
    "    DG = torch.nn.DataParallel(DG).cuda()\n",
    "\n",
    "    dg_optimizer = torch.optim.Adam(DG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    entropy = HLoss()\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        _, unlabel_loader1 = init_dataloader(df=None,data_dir = data_dir,batch_size = batch_size, mode=\"gan\",iterator=True)\n",
    "        _,unlabel_loader2 = init_dataloader(df=None,data_dir =data_dir, batch_size =batch_size, mode=\"gan\",iterator=True)\n",
    "        for i, (imgs,label) in enumerate(dataloader):\n",
    "            current_iter = epoch * len(dataloader) + i + 1\n",
    "\n",
    "            step += 1\n",
    "            imgs = imgs.cuda()\n",
    "            bs = imgs.size(0)\n",
    "            x_unlabel_t = next(unlabel_loader1)\n",
    "            x_unlabel2_t = next(unlabel_loader2)\n",
    "            \n",
    "            freeze(G)\n",
    "            unfreeze(DG)\n",
    "\n",
    "            z = torch.randn(bs, z_dim).cuda()\n",
    "            f_imgs = G(z)\n",
    "\n",
    "            y_prob = T(imgs)[-1]\n",
    "            y = torch.argmax(y_prob, dim=1).view(-1)\n",
    "            \n",
    "            x_unlabel = x_unlabel_t[0]\n",
    "            x_unlabel2 = x_unlabel2_t[0]\n",
    "            _, output_label = DG(imgs)\n",
    "            _, output_unlabel = DG(x_unlabel)\n",
    "            _, output_fake =  DG(f_imgs)\n",
    "\n",
    "            loss_lab = softXEnt(output_label, y_prob)\n",
    "            loss_unlab = 0.5*(torch.mean(F.softplus(log_sum_exp(output_unlabel)))-torch.mean(log_sum_exp(output_unlabel))+torch.mean(F.softplus(log_sum_exp(output_fake))))\n",
    "            dg_loss = loss_lab + loss_unlab\n",
    "            \n",
    "            acc = torch.mean((output_label.max(1)[1] == y).float())\n",
    "            \n",
    "            dg_optimizer.zero_grad()\n",
    "            dg_loss.backward()\n",
    "            dg_optimizer.step()\n",
    "\n",
    "            # train G\n",
    "            if step % n_critic == 0:\n",
    "                freeze(DG)\n",
    "                unfreeze(G)\n",
    "                z = torch.randn(bs, z_dim).cuda()\n",
    "                f_imgs = G(z)\n",
    "                mom_gen, output_fake = DG(f_imgs)\n",
    "                mom_unlabel, _ = DG(x_unlabel2)\n",
    "\n",
    "                mom_gen = torch.mean(mom_gen, dim = 0)\n",
    "                mom_unlabel = torch.mean(mom_unlabel, dim = 0)\n",
    "\n",
    "                Hloss = entropy(output_fake)\n",
    "                g_loss = torch.mean((mom_gen - mom_unlabel).abs()) + 1e-4 * Hloss\n",
    "\n",
    "                g_optimizer.zero_grad()\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "#                 torch.cuda.empty_cache()\n",
    "\n",
    "        end = time.time()\n",
    "        interval = end - start\n",
    "        \n",
    "        print(\"Epoch:%d \\tTime:%.2f\\tG_loss:%.2f\\t train_acc:%.2f\" % (epoch, interval, g_loss, acc))\n",
    "\n",
    "        torch.save({'state_dict':G.state_dict()}, os.path.join(save_model_dir, \"improved_{}_G.pt\".format(dataset_name)))\n",
    "        torch.save({'state_dict':DG.state_dict()}, os.path.join(save_model_dir, \"improved_{}_D.pt\".format(dataset_name)))\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            z = torch.randn(32, z_dim).cuda()\n",
    "            fake_image = G(z)\n",
    "            save_tensor_images(fake_image.detach(), os.path.join(save_img_dir, \"improved_BraTS23_img_{}.png\".format(epoch)), nrow = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d1a880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.768687Z",
     "iopub.status.busy": "2024-11-02T21:50:28.768321Z",
     "iopub.status.idle": "2024-11-02T21:50:28.779534Z",
     "shell.execute_reply": "2024-11-02T21:50:28.778691Z"
    },
    "papermill": {
     "duration": 0.025762,
     "end_time": "2024-11-02T21:50:28.781462",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.755700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#main function of recovery.py\n",
    "def recovery():\n",
    "    improved_flag = True\n",
    "    num_seeds = 1\n",
    "    loss = 'logit_loss'\n",
    "    # args.classid = '0,1,2,3'\n",
    "    root_path = \"/kaggle/working/attack_results\"\n",
    "    create_directory_if_not_exists(root_path)\n",
    "    # Save dir\n",
    "    prefix = os.path.join(root_path, \"kedmi_300ids\") \n",
    "    save_folder = os.path.join(\"{}_{}\".format(\"BraTS23\", \"VGG16_MRI\"), \"L_Logit\")\n",
    "    prefix = os.path.join(prefix, save_folder)\n",
    "    save_dir = os.path.join(prefix, \"latent\")\n",
    "    save_img_dir = os.path.join(prefix, \"imgs_{}\".format(\"L_logit\"))\n",
    "    # args.log_path = os.path.join(prefix, \"invertion_logs\")\n",
    "\n",
    "    os.makedirs(prefix, exist_ok=True)\n",
    "    # os.makedirs(args.log_path, exist_ok=True)\n",
    "    os.makedirs(save_img_dir, exist_ok=True)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    # Load models\n",
    "    targetnets, E, G, D, n_classes, fea_mean, fea_logvar = get_attack_model()\n",
    "    N = 5\n",
    "    bs = 60\n",
    "    \n",
    "\n",
    "    # Begin attacking\n",
    "    for i in range(1):\n",
    "        iden = torch.from_numpy(np.arange(bs))\n",
    "\n",
    "        # evaluate on the first 300 identities only\n",
    "        target_cosines = 0\n",
    "        eval_cosines = 0\n",
    "        for idx in range(5):\n",
    "            iden = iden %n_classes\n",
    "            print(\"--------------------- Attack batch [%s]------------------------------\" % idx)\n",
    "            print('Iden:{}'.format(iden))\n",
    "            save_dir_z = '{}/{}_{}'.format(save_dir,i,idx)\n",
    "            \n",
    "            if True:\n",
    "                #KEDMI\n",
    "                print('kedmi')\n",
    "\n",
    "                dist_inversion(G, D, targetnets, E, iden,  \n",
    "                                        lr=0.02, iter_times=2400,\n",
    "                                        momentum=0.9, lamda=100,  \n",
    "                                        clip_range=1, improved=improved_flag, \n",
    "                                        num_seeds=num_seeds, \n",
    "                                        used_loss=loss,\n",
    "                                        prefix=save_dir_z,\n",
    "                                        save_img_dir=os.path.join(save_img_dir, '{}_'.format(idx)),\n",
    "                                        fea_mean=fea_mean,\n",
    "                                        fea_logvar=fea_logvar,\n",
    "                                        lam=1.0,\n",
    "                                        clipz=True)\n",
    "            iden = iden + bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce9ae90a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.805885Z",
     "iopub.status.busy": "2024-11-02T21:50:28.805608Z",
     "iopub.status.idle": "2024-11-02T21:50:28.808975Z",
     "shell.execute_reply": "2024-11-02T21:50:28.808183Z"
    },
    "papermill": {
     "duration": 0.017899,
     "end_time": "2024-11-02T21:50:28.810822",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.792923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_specific_gan()\n",
    "# recovery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "effb2252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.837497Z",
     "iopub.status.busy": "2024-11-02T21:50:28.836788Z",
     "iopub.status.idle": "2024-11-02T21:50:28.841762Z",
     "shell.execute_reply": "2024-11-02T21:50:28.840887Z"
    },
    "papermill": {
     "duration": 0.020752,
     "end_time": "2024-11-02T21:50:28.843737",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.822985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def show_saved_imgs(image_path):\n",
    "#     image_path = \"/kaggle/working/attack_results/BraTS23/VGG16_MRI/imgs/improved_BraTS23_img_19.png\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccaec64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T21:50:28.868158Z",
     "iopub.status.busy": "2024-11-02T21:50:28.867891Z",
     "iopub.status.idle": "2024-11-02T21:50:28.875847Z",
     "shell.execute_reply": "2024-11-02T21:50:28.874956Z"
    },
    "papermill": {
     "duration": 0.022509,
     "end_time": "2024-11-02T21:50:28.877836",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.855327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_gan():\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        G = Generator(100)\n",
    "        G = torch.nn.DataParallel(G).to(device)\n",
    "        root_path = \"/kaggle/working/attack_results\"\n",
    "        dataset_name = \"BraTS23\"\n",
    "        model_name_T = \"VGG16_MRI\"\n",
    "        path = os.path.join(root_path, os.path.join(dataset_name, model_name_T))\n",
    "        path_G = os.path.join(path, \"improved_{}_G.tar\".format(dataset_name))\n",
    "        ckp_G = torch.load(path_G)\n",
    "        G.load_state_dict(ckp_G['state_dict'], strict=True)\n",
    "        \n",
    "        G.eval()\n",
    "        noise = torch.randn(1, 100)\n",
    "        with torch.no_grad():\n",
    "            generated_image = G(noise)\n",
    "        generated_image = generated_image.squeeze(0).cpu().numpy()\n",
    "        print(generated_image.shape)\n",
    "        # Convert the generated image to a 2D array\n",
    "        generated_image = np.squeeze(generated_image)  # Remove the channel dimension for grayscale\n",
    "\n",
    "        # Plot the generated image\n",
    "        plt.imshow(generated_image, cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis labels\n",
    "        plt.show()\n",
    "# test_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835cd27",
   "metadata": {
    "papermill": {
     "duration": 0.011841,
     "end_time": "2024-11-02T21:50:28.901789",
     "exception": false,
     "start_time": "2024-11-02T21:50:28.889948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5892358,
     "sourceId": 9647841,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 145551,
     "modelInstanceId": 122470,
     "sourceId": 144496,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 152752,
     "modelInstanceId": 129892,
     "sourceId": 152947,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.136924,
   "end_time": "2024-11-02T21:50:30.836280",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-02T21:50:02.699356",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
